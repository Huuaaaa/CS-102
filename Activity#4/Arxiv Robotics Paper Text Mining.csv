"","title","author","subject","abstract","meta"
"1","3D Diffuser Actor: Policy Diffusion with 3D Scene Representations","Tsung-Wei Ke, Nikolaos Gkanatsios, Katerina Fragkiadaki","Robotics (cs.RO)","We marry diffusion policies and 3D scene representations for robot manipulation. Diffusion policies learn the action distribution conditioned on the robot and environment state using conditional diffusion models. They have recently shown to outperform both deterministic and alternative state-conditioned action distribution learning methods. 3D robot policies use 3D scene feature representations aggregated from a single or multiple camera views using sensed depth. They have shown to generalize better than their 2D counterparts across camera viewpoints. We unify these two lines of work and present 3D Diffuser Actor, a neural policy architecture that, given a language instruction, builds a 3D representation of the visual scene and conditions on it to iteratively denoise 3D rotations and translations for the robot's end-effector. At each denoising iteration, our model represents end-effector pose estimates as 3D scene tokens and predicts the 3D translation and rotation error for each of them, by featurizing them using 3D relative attention to other 3D visual and language tokens. 3D Diffuser Actor sets a new state-of-the-art on RLBench with an absolute performance gain of 16.3% over the current SOTA on a multi-view setup and an absolute gain of 13.1% on a single-view setup. On the CALVIN benchmark, it outperforms the current SOTA in the setting of zero-shot unseen scene generalization by being able to successfully run 0.2 more tasks, a 7% relative increase. It also works in the real world from a handful of demonstrations. We ablate our model's architectural design choices, such as 3D scene featurization and 3D relative attentions, and show they all help generalization. Our results suggest that 3D scene representations and powerful generative modeling are keys to efficient robot learning from demonstrations.","Fri, 16 Feb 2024 18:43:02 UTC (27,637 KB)"
"2","Multi-Model 3D Registration: Finding Multiple Moving Objects in Cluttered Point Clouds","David Jin, Sushrut Karmalkar, Harry Zhang, Luca Carlone","Robotics (cs.RO)","We investigate a variation of the 3D registration problem, named multi-model 3D registration. In the multi-model registration problem, we are given two point clouds picturing a set of objects at different poses (and possibly including points belonging to the background) and we want to simultaneously reconstruct how all objects moved between the two point clouds. This setup generalizes standard 3D registration where one wants to reconstruct a single pose, e.g., the motion of the sensor picturing a static scene. Moreover, it provides a mathematically grounded formulation for relevant robotics applications, e.g., where a depth sensor onboard a robot perceives a dynamic scene and has the goal of estimating its own motion (from the static portion of the scene) while simultaneously recovering the motion of all dynamic objects. We assume a correspondence-based setup where we have putative matches between the two point clouds and consider the practical case where these correspondences are plagued with outliers. We then propose a simple approach based on Expectation-Maximization (EM) and establish theoretical conditions under which the EM approach converges to the ground truth. We evaluate the approach in simulated and real datasets ranging from table-top scenes to self-driving scenarios and demonstrate its effectiveness when combined with state-of-the-art scene flow methods to establish dense correspondences.","Fri, 16 Feb 2024 18:01:43 UTC (2,470 KB)"
"3","Pedipulate: Enabling Manipulation Skills using a Quadruped Robot's Leg","Philip Arm, Mayank Mittal, Hendrik Kolvenbach, Marco Hutter","Robotics (cs.RO)","Legged robots have the potential to become vital in maintenance, home support, and exploration scenarios. In order to interact with and manipulate their environments, most legged robots are equipped with a dedicated robot arm, which means additional mass and mechanical complexity compared to standard legged robots. In this work, we explore pedipulation - using the legs of a legged robot for manipulation. By training a reinforcement learning policy that tracks position targets for one foot, we enable a dedicated pedipulation controller that is robust to disturbances, has a large workspace through whole-body behaviors, and can reach far-away targets with gait emergence, enabling loco-pedipulation. By deploying our controller on a quadrupedal robot using teleoperation, we demonstrate various real-world tasks such as door opening, sample collection, and pushing obstacles. We demonstrate load carrying of more than 2.0 kg at the foot. Additionally, the controller is robust to interaction forces at the foot, disturbances at the base, and slippery contact surfaces. Videos of the experiments are available at this https URL.","Fri, 16 Feb 2024 17:20:45 UTC (47,108 KB)"
"4","RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model","Jianhao Yuan, Shuyang Sun, Daniel Omeiza, Bo Zhao, Paul Newman, Lars Kunze, Matthew Gadd","Robotics (cs.RO)","Robots powered by 'blackbox' models need to provide human-understandable explanations which we can trust. Hence, explainability plays a critical role in trustworthy autonomous decision-making to foster transparency and acceptance among end users, especially in complex autonomous driving. Recent advancements in Multi-Modal Large Language models (MLLMs) have shown promising potential in enhancing the explainability as a driving agent by producing control predictions along with natural language explanations. However, severe data scarcity due to expensive annotation costs and significant domain gaps between different datasets makes the development of a robust and generalisable system an extremely challenging task. Moreover, the prohibitively expensive training requirements of MLLM and the unsolved problem of catastrophic forgetting further limit their generalisability post-deployment. To address these challenges, we present RAG-Driver, a novel retrieval-augmented multi-modal large language model that leverages in-context learning for high-performance, explainable, and generalisable autonomous driving. By grounding in retrieved expert demonstration, we empirically validate that RAG-Driver achieves state-of-the-art performance in producing driving action explanations, justifications, and control signal prediction. More importantly, it exhibits exceptional zero-shot generalisation capabilities to unseen environments without further training endeavours.","Fri, 16 Feb 2024 16:57:18 UTC (14,101 KB)"
"5","AutoGPT+P: Affordance-based Task Planning with Large Language Models","Timo Birr, Christoph Pohl, Abdelrahman Younes, Tamim Asfour","Robotics (cs.RO)","Recent advances in task planning leverage Large Language Models (LLMs) to improve generalizability by combining such models with classical planning algorithms to address their inherent limitations in reasoning capabilities. However, these approaches face the challenge of dynamically capturing the initial state of the task planning problem. To alleviate this issue, we propose AutoGPT+P, a system that combines an affordance-based scene representation with a planning system. Affordances encompass the action possibilities of an agent on the environment and objects present in it. Thus, deriving the planning domain from an affordance-based scene representation allows symbolic planning with arbitrary objects. AutoGPT+P leverages this representation to derive and execute a plan for a task specified by the user in natural language. In addition to solving planning tasks under a closed-world assumption, AutoGPT+P can also handle planning with incomplete information, e. g., tasks with missing objects by exploring the scene, suggesting alternatives, or providing a partial plan. The affordance-based scene representation combines object detection with an automatically generated object-affordance-mapping using ChatGPT. The core planning tool extends existing work by automatically correcting semantic and syntactic errors. Our approach achieves a success rate of 98%, surpassing the current 81% success rate of the current state-of-the-art LLM-based planning method SayCan on the SayCan instruction set. Furthermore, we evaluated our approach on our newly created dataset with 150 scenarios covering a wide range of complex tasks with missing objects, achieving a success rate of 79% on our dataset. The dataset and the code are publicly available at this https URL.","Fri, 16 Feb 2024 16:00:50 UTC (3,018 KB)"
"6","A CBF-Adaptive Control Architecture for Visual Navigation for UAV in the Presence of Uncertainties","Viswa Narayanan Sankaranarayanan, Akshit Saradagi, Sumeet Satpute, George Nikolakopoulos","Robotics (cs.RO)","In this article, we propose a control solution for the safe transfer of a quadrotor UAV between two surface robots positioning itself only using the visual features on the surface robots, which enforces safety constraints for precise landing and visual locking, in the presence of modeling uncertainties and external disturbances. The controller handles the ascending and descending phases of the navigation using a visual locking control barrier function (VCBF) and a parametrizable switching descending CBF (DCBF) respectively, eliminating the need for an external planner. The control scheme has a backstepping approach for the position controller with the CBF filter acting on the position kinematics to produce a filtered virtual velocity control input, which is tracked by an adaptive controller to overcome modeling uncertainties and external disturbances. The experimental validation is carried out with a UAV that navigates from the base to the target using an RGB camera.","Fri, 16 Feb 2024 14:47:39 UTC (2,972 KB)"
"7","StableLego: Stability Analysis of Block Stacking Assembly","Ruixuan Liu, Kangle Deng, Ziwei Wang, Changliu Liu","Robotics (cs.RO)","Recent advancements in robotics enable robots to accomplish complex assembly tasks. However, designing an assembly requires a non-trivial effort since a slight variation in the design could significantly affect the task feasibility. It is critical to ensure the physical feasibility of the assembly design so that the assembly task can be successfully executed. To address the challenge, this paper studies the physical stability of assembly structures, in particular, block stacking assembly, where people use cubic blocks to build 3D structures (e.g., Lego constructions). The paper proposes a new optimization formulation, which optimizes over force balancing equations, for inferring the structural stability of 3D block-stacking structures. The proposed stability analysis is tested and verified on hand-crafted Lego examples. The experiment results demonstrate that the proposed stability analysis can correctly predict whether the structure is stable. In addition, it outperforms the existing methods since it can locate the weakest parts in the design, and more importantly, solve any given assembly structure. To further validate the proposed analysis formulation, we provide StableLego: a comprehensive dataset including more than 50k 3D objects with their Lego layouts. We test the proposed stability analysis and include the stability inference for each corresponding object in StableLego. Our code and the dataset are available at this https URL.","Fri, 16 Feb 2024 14:14:23 UTC (6,507 KB)"
"8","OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via Vision-Language Foundation Models","Yuxuan Kuang, Hai Lin, Meng Jiang","Computation and Language (cs.CL)","Object navigation (ObjectNav) requires an agent to navigate through unseen environments to find queried objects. Many previous methods attempted to solve this task by relying on supervised or reinforcement learning, where they are trained on limited household datasets with close-set objects. However, two key challenges are unsolved: understanding free-form natural language instructions that demand open-set objects, and generalizing to new environments in a zero-shot manner. Aiming to solve the two challenges, in this paper, we propose OpenFMNav, an Open-set Foundation Model based framework for zero-shot object Navigation. We first unleash the reasoning abilities of large language models (LLMs) to extract proposed objects from natural language instructions that meet the user's demand. We then leverage the generalizability of large vision language models (VLMs) to actively discover and detect candidate objects from the scene, building a Versatile Semantic Score Map (VSSM). Then, by conducting common sense reasoning on VSSM, our method can perform effective language-guided exploration and exploitation of the scene and finally reach the goal. By leveraging the reasoning and generalizing abilities of foundation models, our method can understand free-form human instructions and perform effective open-set zero-shot navigation in diverse environments. Extensive experiments on the HM3D ObjectNav benchmark show that our method surpasses all the strong baselines on all metrics, proving our method's effectiveness. Furthermore, we perform real robot demonstrations to validate our method's open-set-ness and generalizability to real-world environments.","Fri, 16 Feb 2024 13:21:33 UTC (12,905 KB)"
"9","An energy-based material model for the simulation of shape memory alloys under complex boundary value problems","C. Erdogan, T. Bode, P. Junker","Computational Engineering, Finance, and Science (cs.CE)","Shape memory alloys are remarkable 'smart' materials used in a broad spectrum of applications, ranging from aerospace to robotics, thanks to their unique thermomechanical coupling capabilities. Given the complex properties of shape memory alloys, which are largely influenced by thermal and mechanical loads, as well as their loading history, predicting their behavior can be challenging. Consequently, there exists a pronounced demand for an efficient material model to simulate the behavior of these alloys. This paper introduces a material model rooted in Hamilton's principle. The key advantages of the presented material model encompass a more accurate depiction of the internal variable evolution and heightened robustness. As such, the proposed material model signifies an advancement in the realistic and efficient simulation of shape memory alloys.","Fri, 16 Feb 2024 13:03:18 UTC (18,962 KB)"
"10","Precise Hybrid-Actuation Robotic Fiber for Enhanced Cervical Disease Treatment","Jinshi Zhao, Qindong Zheng, Ali Anil Demircali, Xiaotong Guo, Daniel Simon, Maria Paraskevaidi, Nick W F Linton, Zoltan Takats, Maria Kyrgiou, Burak Temelkuran","Robotics (cs.RO)","Treatment for high-grade precancerous cervical lesions and early-stage cancers, mainly affecting women of reproductive age, often involves fertility-sparing treatment methods. Commonly used local treatments for cervical precancers have shown the risk of leaving a positive cancer margin and engendering subsequent complications according to the precision and depth of excision. An intra-operative device that allows the careful excision of the disease while conserving healthy cervical tissue would potentially enhance such treatment. In this study, we developed a polymer-based robotic fiber measuring 150 mm in length and 1.7 mm in diameter, fabricated using a highly scalable fiber drawing technique. This robotic fiber utilizes a hybrid actuation mechanism, combining electrothermal and tendon-driven actuation mechanisms, thus enabling a maximum motion range of 46 mm from its origin with a sub-100 {\mu}m motion precision. We also developed control algorithms for the actuation methods of this robotic fiber, including predefined path control and telemanipulation, enabling coarse positioning of the fiber tip to the target area followed by a precise scan. The combination of a surgical laser fiber with the robotic fiber allows for high-precision surgical ablation. Additionally, we conducted experiments using a cervical phantom that demonstrated the robotic fiber's ability to access and perform high-precision scans, highlighting its potential for cervical disease treatments and improvement of oncological outcomes.","Fri, 16 Feb 2024 10:49:38 UTC (1,025 KB)"
